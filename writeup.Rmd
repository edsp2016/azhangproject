---
title: "What's that makes NYC attractive?"
output: html_document
---
According to E.Currid's research, from an economics development perspective NYC's comparative advantage is the Arts rather than finance [^1]. I wonder what is the most and least attractive things of NYC from the perspective of the mass. I decide to explore this idea by analyzing data from Twitter. By sharing and commenting tweets, Twitter creates a platform for collective experience and a way for urban dewellers to document their interactions with the city. Therefore, Twitter is a great resource to explore for my topic: what makes NYC attractive and not attractive. What is the most and least attractive things of NYC comparing with other cities, like LA? 

This research would help urban planners and social policy decision makers to identify the most and least attractive things of NYC and make better policies to help develop the attractive things and improve the least attractive things.

I will collect Tweets which contain hashtags #NYC and #LA and data with geotag NYC and LA. I will categorize tweets to positive and negative for NYC and LA. I will make comparisons between NYC and LA: Positive and negative Tweets about NYC but outside of NYC and positive and negative Tweets tweeted in NYC. Same for LA. I would also like to see if the other words and hashtags are associated with each other, and the amount of likes and retweets are assosiated with the tweets. 

I would like to make word frequency maps of specific trending topics of NYC and LA over time and make a short movie of it.

I need to develop skills in machine learning, natural language processing, crawling techniques to obtain data from Twitter, and interactive map making techniques with D3. The measurement for success is if I'm able to identify the attractive things of NYC on Instagram. 

I'm now working on writing codes to get data from twitter constantly, because there is a limit for getting twitter data.



```{r}
install.packages(c("devtools", "rjson", "bit64", "httr"))
library(devtools)
install_github("twitteR", username="geoffjentry")
library(twitteR)
devtools::install_github("jrowen/twitteR", ref = "oauth_httr_1_0")

api_key <- "1Z5dXmHGGMHnYQPsmO66vTweS"
api_secret <- "DbuXQ8jISNDzBKDl9MXAAJmfYg37PyTNK4nGOoHq66sb3QBZDX"
access_token <- "457034907-YDKNiNBwLaxGpylZtQ61hF9afL7yhj6o1rQ3nqn5"
access_token_secret <-"VziI7op8XMqep65qiOPkD1eatodQU6EwEVMMyXzF7IVHG"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)

nyc = searchTwitter("#nyc")
test=unlist(nyc)
test
dm2_tweets = searchTwitter("NYC")
test1=sapply(nyc, function(x) list(text = x$screenName, id = x$id))
test2=as.data.frame(t(test1))
as.data.frame(t(sapply(dm2_tweets, function(x) list(text = x$screenName, id = x$id))))
# search tweets containing "data mining"
dm1_tweets = searchTwitter("NYC",geocode='-41,74,1mi')

f <- function(data){
  temp=searchTwitter("#nyc")
  data =  as.data.frame(t(sapply(temp, function(x) list(text = x$screenName, id = x$id))))
  return(data)
}

for(i in 1:24){
  for(i in 1:24){
    master.df <- f(master.df)
    Sys.sleep(20*60)
    }
  setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
}

```

```{r}
#2459115 is the woeif for NYC
  pulledData =  as.data.frame(getTrends(2459115))
```


```{r}
f <- function(data,place){
  pulledData =  as.data.frame(getTrends(place$woeid))
  pulledData = cbind(pulledData,city = place$nam,latitude =  place$lat,longitude = place$lon, timestamp = Sys.time())
  combinedData <- rbind(data,pulledData)
  return(combinedData)
}

for(i in 1:24){
  for(i in 1:24){
    x = 1
    y = 16
    z = 31
    for(i in 1:15){
      # 1 to 15
      master.df <- f(master.df,locs[(x),])
      x = x+1
    }
    Sys.sleep(20*60)
    master.df <- f(master.df, locs[(z),])
    Sys.sleep(20*60)
    }
  setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
}
```


```{r}
#look into twitterR
filterStream(file.name="tweets_geo.json", locations=c(-74, 40, -73, 41),timeout=60, oauth=my_oauth)
tweets <- parseTweets("tweets_geo.json")
library(maps)
tweets$lat <- ifelse(is.na(tweets$lat), tweets$place_lat, tweets$lat)
tweets$lon <- ifelse(is.na(tweets$lon), tweets$place_lon, tweets$lon)
states <- map.where("state", tweets$lon, tweets$lat)
head(sort(table(states), decreasing=TRUE))
#install.packages("ggplot2")
library(ggplot2)

## First create a data frame with the map data 
map.data <- map_data("state")

# And we use ggplot2 to draw the map:
# 1) map base
ggplot(map.data) + geom_map(aes(map_id = region), map = map.data, fill = "grey90", 
    color = "grey50", size = 0.25) + expand_limits(x = map.data$long, y = map.data$lat) + 
    # 2) limits for x and y axis
    scale_x_continuous(limits=c(-125,-66)) + scale_y_continuous(limits=c(25,50)) +
    # 3) adding the dot for each tweet
    geom_point(data = tweets, 
    aes(x = lon, y = lat), size = 1, alpha = 1/5, color = "darkblue") +
    # 4) removing unnecessary graph elements
    theme(axis.line = element_blank(), 
    	axis.text = element_blank(), 
    	axis.ticks = element_blank(), 
        axis.title = element_blank(), 
        panel.background = element_blank(), 
        panel.border = element_blank(), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        plot.background = element_blank()) 

```


[^1]: Elizabeth Currid, "New York as a Global Creative Hub: A Competitive Analysis of Four Theories on World Cities," *Economic Development Quarrterly*, Vol.20 No.4, November 2006:344.
[^2]:Amar K. Boghani, "The City Expressed: Everyday Media Production and the Urban Enviroment", https://www.scribd.com/fullscreen/178445235?access_key=key-25xzcu2ng2h4qi45mg89&allow_share=true&escape=false&view_mode=scroll
